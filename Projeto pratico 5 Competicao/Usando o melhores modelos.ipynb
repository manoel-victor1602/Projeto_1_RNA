{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_data.csv')\n",
    "df_test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>82</td>\n",
       "      <td>5.663</td>\n",
       "      <td>1.145436</td>\n",
       "      <td>35.5900</td>\n",
       "      <td>26.720000</td>\n",
       "      <td>4.58000</td>\n",
       "      <td>174.800</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>43</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>103</td>\n",
       "      <td>4.328</td>\n",
       "      <td>1.099601</td>\n",
       "      <td>25.7816</td>\n",
       "      <td>12.718960</td>\n",
       "      <td>38.65310</td>\n",
       "      <td>775.322</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "      <td>27.887617</td>\n",
       "      <td>99</td>\n",
       "      <td>9.208</td>\n",
       "      <td>2.248594</td>\n",
       "      <td>12.6757</td>\n",
       "      <td>5.478170</td>\n",
       "      <td>23.03306</td>\n",
       "      <td>407.206</td>\n",
       "      <td>2</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>20.829995</td>\n",
       "      <td>74</td>\n",
       "      <td>4.560</td>\n",
       "      <td>0.832352</td>\n",
       "      <td>7.7529</td>\n",
       "      <td>8.237405</td>\n",
       "      <td>28.03230</td>\n",
       "      <td>382.955</td>\n",
       "      <td>2</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>46</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>88</td>\n",
       "      <td>3.420</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>12.8700</td>\n",
       "      <td>18.550000</td>\n",
       "      <td>13.56000</td>\n",
       "      <td>301.210</td>\n",
       "      <td>2</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>90</td>\n",
       "      <td>2.540</td>\n",
       "      <td>0.563880</td>\n",
       "      <td>15.5325</td>\n",
       "      <td>10.222310</td>\n",
       "      <td>16.11032</td>\n",
       "      <td>1698.440</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>20.956608</td>\n",
       "      <td>94</td>\n",
       "      <td>12.305</td>\n",
       "      <td>2.853119</td>\n",
       "      <td>11.2406</td>\n",
       "      <td>8.412175</td>\n",
       "      <td>23.11770</td>\n",
       "      <td>573.630</td>\n",
       "      <td>2</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93</td>\n",
       "      <td>49</td>\n",
       "      <td>32.461911</td>\n",
       "      <td>134</td>\n",
       "      <td>24.887</td>\n",
       "      <td>8.225983</td>\n",
       "      <td>42.3914</td>\n",
       "      <td>10.793940</td>\n",
       "      <td>5.76800</td>\n",
       "      <td>656.393</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63</td>\n",
       "      <td>51</td>\n",
       "      <td>22.892820</td>\n",
       "      <td>103</td>\n",
       "      <td>2.740</td>\n",
       "      <td>0.696143</td>\n",
       "      <td>8.0163</td>\n",
       "      <td>9.349775</td>\n",
       "      <td>11.55492</td>\n",
       "      <td>359.232</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>36.790166</td>\n",
       "      <td>101</td>\n",
       "      <td>10.175</td>\n",
       "      <td>2.534932</td>\n",
       "      <td>27.1841</td>\n",
       "      <td>20.030000</td>\n",
       "      <td>10.26309</td>\n",
       "      <td>695.754</td>\n",
       "      <td>1</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>34.838148</td>\n",
       "      <td>95</td>\n",
       "      <td>12.548</td>\n",
       "      <td>2.940415</td>\n",
       "      <td>33.1612</td>\n",
       "      <td>2.364950</td>\n",
       "      <td>9.95420</td>\n",
       "      <td>655.834</td>\n",
       "      <td>2</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>89</td>\n",
       "      <td>58</td>\n",
       "      <td>29.154519</td>\n",
       "      <td>139</td>\n",
       "      <td>16.582</td>\n",
       "      <td>5.685415</td>\n",
       "      <td>22.8884</td>\n",
       "      <td>10.262660</td>\n",
       "      <td>13.97399</td>\n",
       "      <td>923.886</td>\n",
       "      <td>2</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>61</td>\n",
       "      <td>32.038959</td>\n",
       "      <td>85</td>\n",
       "      <td>18.077</td>\n",
       "      <td>3.790144</td>\n",
       "      <td>30.7729</td>\n",
       "      <td>7.780255</td>\n",
       "      <td>13.68392</td>\n",
       "      <td>444.395</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>22.656250</td>\n",
       "      <td>92</td>\n",
       "      <td>3.482</td>\n",
       "      <td>0.790182</td>\n",
       "      <td>9.8648</td>\n",
       "      <td>11.236235</td>\n",
       "      <td>10.69548</td>\n",
       "      <td>703.973</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>22.656250</td>\n",
       "      <td>92</td>\n",
       "      <td>3.482</td>\n",
       "      <td>0.790182</td>\n",
       "      <td>9.8648</td>\n",
       "      <td>11.236235</td>\n",
       "      <td>10.69548</td>\n",
       "      <td>703.973</td>\n",
       "      <td>2</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>64</td>\n",
       "      <td>34.529723</td>\n",
       "      <td>95</td>\n",
       "      <td>4.427</td>\n",
       "      <td>1.037394</td>\n",
       "      <td>21.2117</td>\n",
       "      <td>5.462620</td>\n",
       "      <td>6.70188</td>\n",
       "      <td>252.449</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30</td>\n",
       "      <td>66</td>\n",
       "      <td>36.212279</td>\n",
       "      <td>101</td>\n",
       "      <td>15.533</td>\n",
       "      <td>3.869788</td>\n",
       "      <td>74.7069</td>\n",
       "      <td>7.539550</td>\n",
       "      <td>22.32024</td>\n",
       "      <td>864.968</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>21.082813</td>\n",
       "      <td>102</td>\n",
       "      <td>6.200</td>\n",
       "      <td>1.559920</td>\n",
       "      <td>9.6994</td>\n",
       "      <td>8.574655</td>\n",
       "      <td>13.74244</td>\n",
       "      <td>448.799</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>73</td>\n",
       "      <td>72</td>\n",
       "      <td>23.620000</td>\n",
       "      <td>105</td>\n",
       "      <td>4.420</td>\n",
       "      <td>1.144780</td>\n",
       "      <td>21.7800</td>\n",
       "      <td>17.860000</td>\n",
       "      <td>4.82000</td>\n",
       "      <td>195.940</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>97</td>\n",
       "      <td>3.350</td>\n",
       "      <td>0.801543</td>\n",
       "      <td>4.4700</td>\n",
       "      <td>10.358725</td>\n",
       "      <td>6.28445</td>\n",
       "      <td>136.855</td>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>46</td>\n",
       "      <td>75</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>94</td>\n",
       "      <td>8.079</td>\n",
       "      <td>1.873251</td>\n",
       "      <td>65.9260</td>\n",
       "      <td>3.741220</td>\n",
       "      <td>4.49685</td>\n",
       "      <td>206.802</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>83</td>\n",
       "      <td>4.952</td>\n",
       "      <td>1.013839</td>\n",
       "      <td>17.1270</td>\n",
       "      <td>11.578990</td>\n",
       "      <td>7.09130</td>\n",
       "      <td>318.302</td>\n",
       "      <td>1</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>115</td>\n",
       "      <td>86</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>138</td>\n",
       "      <td>19.910</td>\n",
       "      <td>6.777364</td>\n",
       "      <td>90.2800</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>4.35000</td>\n",
       "      <td>90.090</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  \\\n",
       "0      11   29  23.010000       82    5.663  1.145436  35.5900    26.720000   \n",
       "1      77   43  31.250000      103    4.328  1.099601  25.7816    12.718960   \n",
       "2      96   44  27.887617       99    9.208  2.248594  12.6757     5.478170   \n",
       "3      53   45  20.829995       74    4.560  0.832352   7.7529     8.237405   \n",
       "4      68   46  20.830000       88    3.420  0.742368  12.8700    18.550000   \n",
       "5      86   48  28.125000       90    2.540  0.563880  15.5325    10.222310   \n",
       "6      54   49  20.956608       94   12.305  2.853119  11.2406     8.412175   \n",
       "7      93   49  32.461911      134   24.887  8.225983  42.3914    10.793940   \n",
       "8      63   51  22.892820      103    2.740  0.696143   8.0163     9.349775   \n",
       "9      31   53  36.790166      101   10.175  2.534932  27.1841    20.030000   \n",
       "10    104   57  34.838148       95   12.548  2.940415  33.1612     2.364950   \n",
       "11     89   58  29.154519      139   16.582  5.685415  22.8884    10.262660   \n",
       "12     17   61  32.038959       85   18.077  3.790144  30.7729     7.780255   \n",
       "13     59   62  22.656250       92    3.482  0.790182   9.8648    11.236235   \n",
       "14     59   62  22.656250       92    3.482  0.790182   9.8648    11.236235   \n",
       "15     18   64  34.529723       95    4.427  1.037394  21.2117     5.462620   \n",
       "16     30   66  36.212279      101   15.533  3.869788  74.7069     7.539550   \n",
       "17     57   68  21.082813      102    6.200  1.559920   9.6994     8.574655   \n",
       "18     73   72  23.620000      105    4.420  1.144780  21.7800    17.860000   \n",
       "19      8   73  22.000000       97    3.350  0.801543   4.4700    10.358725   \n",
       "20     46   75  25.700000       94    8.079  1.873251  65.9260     3.741220   \n",
       "21      9   75  23.000000       83    4.952  1.013839  17.1270    11.578990   \n",
       "22    115   86  27.180000      138   19.910  6.777364  90.2800    14.110000   \n",
       "\n",
       "    Resistin     MCP.1  Classification     id  \n",
       "0    4.58000   174.800               1   78.0  \n",
       "1   38.65310   775.322               2   48.0  \n",
       "2   23.03306   407.206               2  113.0  \n",
       "3   28.03230   382.955               2   59.0  \n",
       "4   13.56000   301.210               2   53.0  \n",
       "5   16.11032  1698.440               2   31.0  \n",
       "6   23.11770   573.630               2   89.0  \n",
       "7    5.76800   656.393               2   11.0  \n",
       "8   11.55492   359.232               2   14.0  \n",
       "9   10.26309   695.754               1   84.0  \n",
       "10   9.95420   655.834               2   63.0  \n",
       "11  13.97399   923.886               2   71.0  \n",
       "12  13.68392   444.395               1   58.0  \n",
       "13  10.69548   703.973               2  100.0  \n",
       "14  10.69548   703.973               2   76.0  \n",
       "15   6.70188   252.449               1   85.0  \n",
       "16  22.32024   864.968               1   56.0  \n",
       "17  13.74244   448.799               2   21.0  \n",
       "18   4.82000   195.940               2   60.0  \n",
       "19   6.28445   136.855               1   86.0  \n",
       "20   4.49685   206.802               1   77.0  \n",
       "21   7.09130   318.302               1  112.0  \n",
       "22   4.35000    90.090               2   36.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No axis named 1 for object type <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-95e104b9e0f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3414\u001b[0m         return super(Series, self).drop(labels=labels, axis=axis, index=index,\n\u001b[0;32m   3415\u001b[0m                                         \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3416\u001b[1;33m                                         inplace=inplace, errors=errors)\n\u001b[0m\u001b[0;32m   3417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3418\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mSubstitution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3097\u001b[0m                 raise ValueError(\"Cannot specify both 'labels' and \"\n\u001b[0;32m   3098\u001b[0m                                  \"'index'/'columns'\")\n\u001b[1;32m-> 3099\u001b[1;33m             \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3100\u001b[0m             \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3101\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_axis_name\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    386\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         raise ValueError('No axis named {0} for object type {1}'\n\u001b[1;32m--> 388\u001b[1;33m                          .format(axis, type(self)))\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No axis named 1 for object type <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "X_train = df_train.iloc[:, :-1]\n",
    "y_train = df_train.iloc[:, -1]\n",
    "X_test = df_test.iloc[:, :-1].drop('index', axis=1)\n",
    "y_test = df_test.iloc[:, -1].drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = defaultdict()\n",
    "arq = open('best_models.txt', 'rb')\n",
    "best_models = pickle.load(arq)\n",
    "arq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'models': [['lbfgs', 'logistic', (2,), 0.0005, 600],\n",
       "              ['lbfgs', 'logistic', (2,), 0.003, 800],\n",
       "              ['lbfgs', 'logistic', (3, 3), 0.0005, 1000],\n",
       "              ['lbfgs', 'relu', (1, 8), 0.001, 600],\n",
       "              ['lbfgs', 'relu', (2,), 0.0005, 800],\n",
       "              ['lbfgs', 'tanh', (5, 1), 0.001, 1000]],\n",
       "             'y_preds': [array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1,\n",
       "                     1, 2], dtype=int64),\n",
       "              array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1,\n",
       "                     1, 2], dtype=int64),\n",
       "              array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1,\n",
       "                     1, 1], dtype=int64),\n",
       "              array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1,\n",
       "                     1, 1], dtype=int64),\n",
       "              array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1,\n",
       "                     1, 1], dtype=int64),\n",
       "              array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1,\n",
       "                     1, 2], dtype=int64)],\n",
       "             'cms': [array([ 8,  1,  0, 15]),\n",
       "              array([ 8,  1,  0, 15]),\n",
       "              array([ 9,  0,  1, 14]),\n",
       "              array([ 9,  0,  1, 14]),\n",
       "              array([ 9,  0,  1, 14]),\n",
       "              array([ 8,  1,  0, 15])],\n",
       "             'accuracies': [95.83, 95.83, 95.83, 95.83, 95.83, 95.83],\n",
       "             'f1_scores': [94.12, 94.12, 94.74, 94.74, 94.74, 94.12]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for s, a, h, l, e in best_models['models']:\n",
    "    models.append(MLPClassifier(solver=s, activation=a, hidden_layer_sizes=h, learning_rate_init=l, max_iter=e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([ 417.114,  468.786,  554.697,  928.22 ,  773.92 ,  530.41 ,\n        280.694,  354.6  ,  313.73 ,  632.22 ,  165.02 ,   63.61 ,\n        191.72 ,  588.46 ,  534.224,  572.783,  904.981,  733.797,\n       1227.91 ,  586.173,  887.16 , 1102.11 ,  667.928,  581.313,\n        358.624,  960.246,  473.859,  585.307,  634.602,  263.499,\n        378.996,  618.272,  698.789,  377.227,  335.393,  270.142,\n        200.976,  225.88 ,  209.749,  215.769,  232.006,   45.843,\n        488.829,  552.444,  481.949,  321.919,   90.6  ,  199.055,\n        713.239,  737.672,  355.31 ,  518.586,  635.049,  395.976,\n        220.66 ,  193.87 ,  244.75 ,  513.66 ,  312.   ,  806.724,\n        483.377,  783.796,  910.489,  426.175,  738.034,  799.898,\n       1041.843, 1698.44 , 1078.359, 1698.44 ,  638.261,  994.316,\n        764.667,  396.021,  602.486,  293.123,  256.001,  353.568,\n        572.401,  269.487,  396.648,  232.018,  788.902,  621.273,\n        209.19 ,  198.4  ,   99.45 ,  218.28 ,  268.23 ,  330.16 ,\n        314.05 ,  392.46 ]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2e7ac2e04b91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \"\"\"\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    328\u001b[0m                              hidden_layer_sizes)\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_input_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (array([ 417.114,  468.786,  554.697,  928.22 ,  773.92 ,  530.41 ,\n        280.694,  354.6  ,  313.73 ,  632.22 ,  165.02 ,   63.61 ,\n        191.72 ,  588.46 ,  534.224,  572.783,  904.981,  733.797,\n       1227.91 ,  586.173,  887.16 , 1102.11 ,  667.928,  581.313,\n        358.624,  960.246,  473.859,  585.307,  634.602,  263.499,\n        378.996,  618.272,  698.789,  377.227,  335.393,  270.142,\n        200.976,  225.88 ,  209.749,  215.769,  232.006,   45.843,\n        488.829,  552.444,  481.949,  321.919,   90.6  ,  199.055,\n        713.239,  737.672,  355.31 ,  518.586,  635.049,  395.976,\n        220.66 ,  193.87 ,  244.75 ,  513.66 ,  312.   ,  806.724,\n        483.377,  783.796,  910.489,  426.175,  738.034,  799.898,\n       1041.843, 1698.44 , 1078.359, 1698.44 ,  638.261,  994.316,\n        764.667,  396.021,  602.486,  293.123,  256.001,  353.568,\n        572.401,  269.487,  396.648,  232.018,  788.902,  621.273,\n        209.19 ,  198.4  ,   99.45 ,  218.28 ,  268.23 ,  330.16 ,\n        314.05 ,  392.46 ]),)"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "while(acc < 0.95):\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "        print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1,\n",
       "       1, 2], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_test.id, pd.Series(y_pred)], axis=1, keys=['id', 'Classification']).to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
